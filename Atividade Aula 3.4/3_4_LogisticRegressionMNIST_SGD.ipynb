{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.4-LogisticRegressionMNIST-SGD.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbTOc6cbl-eq"
      },
      "source": [
        "# Regressão Softmax com dados do MNIST utilizando gradiente descendente estocástico por minibatches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-IK0yRml-er"
      },
      "source": [
        "O objetivo deste notebook é ilustrar\n",
        "- o uso do gradiente estocástico por mini-batchs\n",
        "- utilizando as classes Dataset e DataLoater.\n",
        "\n",
        "A apresentação da perda nos gráficos é um pouco diferente da usual, mostrando a perda de cada um dos vários minibatches dentro de cada época, de forma que as épocas são apresentadas com valores fracionários."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqqDXfr4l-et"
      },
      "source": [
        "## Importação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVjLNfp3dDnm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "742fd2f1-2c26-4a66-f482-f94c690de4bd"
      },
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "#!pip install numpy==1.15.0\n",
        "!pip install -q install wheel==0.34.2 setuptools\n",
        "\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "print(get_abbr_impl())\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "#https://download.pytorch.org/whl/cpu/torch-0.4.1.post2-cp37-cp37m-linux_x86_64.whl\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1.post2-{platform}-linux_x86_64.whl torchvision==0.2.1\n",
        "#http://download.pytorch.org/whl/{accelerator}/torch-0.4.1.post2-{platform}-linux_x86_64.whl\n",
        "import torch"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp\n",
            "\u001b[31m  ERROR: HTTP error 403 while getting http://download.pytorch.org/whl/cpu/torch-0.4.1.post2-cp310-cp310-linux_x86_64.whl\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not install requirement torch==0.4.1.post2 from http://download.pytorch.org/whl/cpu/torch-0.4.1.post2-cp310-cp310-linux_x86_64.whl because of HTTP error 403 Client Error: Forbidden for url: http://download.pytorch.org/whl/cpu/torch-0.4.1.post2-cp310-cp310-linux_x86_64.whl for URL http://download.pytorch.org/whl/cpu/torch-0.4.1.post2-cp310-cp310-linux_x86_64.whl\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T22:39:50.314578Z",
          "start_time": "2017-11-24T22:39:48.904460Z"
        },
        "id": "0Iry5pajl-eu"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqgILyIkl-ey"
      },
      "source": [
        "## Dataset e dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAMBUN5ol-ez"
      },
      "source": [
        "### Definição do tamanho do minibatch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T22:39:50.323379Z",
          "start_time": "2017-11-24T22:39:50.318024Z"
        },
        "id": "1YGmeZtbl-ez"
      },
      "source": [
        "batch_size = 100"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uaaezsvl-e5"
      },
      "source": [
        "### Carregamento, criação dataset e do dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cuFYxtrmT45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b0c172f-b165-4105-b01c-b2a364102b1e"
      },
      "source": [
        "! git clone https://github.com/vcasadei/MNIST.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MNIST'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Total 10 (delta 0), reused 0 (delta 0), pack-reused 10\u001b[K\n",
            "Receiving objects: 100% (10/10), 11.01 MiB | 9.87 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T22:39:50.400325Z",
          "start_time": "2017-11-24T22:39:50.326019Z"
        },
        "id": "TRXgajEXl-e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1546ac49-a012-4e6c-b0e9-6c65f324feb3"
      },
      "source": [
        "dataset_dir = 'MNIST/'\n",
        "\n",
        "dataset_train = MNIST(dataset_dir, train=True, download=True,\n",
        "                      transform=torchvision.transforms.ToTensor())\n",
        "\n",
        "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print('Número de minibatches de trenamento:', len(loader_train))\n",
        "\n",
        "x_train, y_train = next(iter(loader_train))\n",
        "print(\"\\nDimensões dos dados de um minibatch:\", x_train.size())\n",
        "print(\"Valores mínimo e máximo dos pixels: \", torch.min(x_train), torch.max(x_train))\n",
        "print(\"Tipo dos dados das imagens:         \", type(x_train))\n",
        "print(\"Tipo das classes das imagens:       \", type(y_train))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 19860518.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST/MNIST/raw/train-images-idx3-ubyte.gz to MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 734266.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4689725.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 8014526.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de minibatches de trenamento: 600\n",
            "\n",
            "Dimensões dos dados de um minibatch: torch.Size([100, 1, 28, 28])\n",
            "Valores mínimo e máximo dos pixels:  tensor(0.) tensor(1.)\n",
            "Tipo dos dados das imagens:          <class 'torch.Tensor'>\n",
            "Tipo das classes das imagens:        <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jalws0jHl-fB"
      },
      "source": [
        "### Usando apenas 1000 amostras do MNIST\n",
        "\n",
        "Neste exemplo utilizaremos 1000 amostras de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T22:39:50.414117Z",
          "start_time": "2017-11-24T22:39:50.402687Z"
        },
        "id": "TnFMMGDKl-fC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cb4e4e8-b1a4-4b46-8236-84cb2ff3f560"
      },
      "source": [
        "if False:\n",
        "    n_samples_train = 1000\n",
        "\n",
        "    dataset_train.train_data = dataset_train.train_data[:n_samples_train]\n",
        "    dataset_train.train_labels = dataset_train.train_labels[:n_samples_train]\n",
        "\n",
        "print('Número de minibatches de trenamento:', len(loader_train))\n",
        "n_batches_train = len(loader_train)\n",
        "total_samples = dataset_train.train_data.size(0)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de minibatches de trenamento: 600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:76: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz6ke_hfl-fF"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T22:39:50.422994Z",
          "start_time": "2017-11-24T22:39:50.416568Z"
        },
        "id": "8iCWX43fl-fG"
      },
      "source": [
        "model = torch.nn.Linear(28*28, 10)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrIFjo18dZ_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8cdf5ef-6aa7-447e-9c72-fdd557cd2496"
      },
      "source": [
        "x = torch.ones(28*28).reshape(1, 784)\n",
        "print(x.shape)\n",
        "predict = model(x)\n",
        "predict"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 784])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6.9828e-01,  8.3975e-01,  4.8076e-01, -5.5119e-01, -6.8694e-04,\n",
              "          3.1965e-01,  1.2462e+00,  1.0204e+00,  5.2247e-01,  2.2351e-01]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVDFEZBql-fJ"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUAeuSK6l-fJ"
      },
      "source": [
        "### Inicialização dos parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T22:39:50.433321Z",
          "start_time": "2017-11-24T22:39:50.426167Z"
        },
        "id": "XT5_fSiQl-fK"
      },
      "source": [
        "n_epochs = 50\n",
        "learningRate = 0.5\n",
        "\n",
        "# Utilizaremos CrossEntropyLoss como função de perda\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Gradiente descendente\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OM9OWI6l-fN"
      },
      "source": [
        "### Laço de treinamento dos parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T22:39:50.984972Z",
          "start_time": "2017-11-24T22:39:50.435673Z"
        },
        "id": "JvNVUUFTl-fQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "9fda7e9a-95b5-4167-cda4-890c7ff00e00"
      },
      "source": [
        "epochs = []\n",
        "losses = []\n",
        "\n",
        "total_trained_samples = 0\n",
        "for i in range(n_epochs):\n",
        "    for k,(x_train, y_train) in enumerate(loader_train):\n",
        "        # Transforma a entrada para uma dimensão\n",
        "        inputs = Variable(x_train.view(-1, 28 * 28))\n",
        "        # predict da rede\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # calcula a perda\n",
        "        loss = criterion(outputs, Variable(y_train))\n",
        "\n",
        "        # zero, backpropagation, ajusta parâmetros pelo gradiente descendente\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_trained_samples += x_train.size(0)\n",
        "        epochs.append(total_trained_samples / total_samples)\n",
        "        losses.append(loss.data[0])\n",
        "        #print(k)\n",
        "    print(f'Época: {i}/{n_epochs-1} minibatch: {k}')#.format(i,n_epochs-1))#, end='\\r')\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-86be121b62de>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtotal_trained_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_trained_samples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m#print(k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Época: {i}/{n_epochs-1} minibatch: {k}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.format(i,n_epochs-1))#, end='\\r')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T22:39:50.992302Z",
          "start_time": "2017-11-24T22:39:50.987284Z"
        },
        "id": "aYO4bgVwl-fT"
      },
      "source": [
        "print('Final loss:', loss.data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEhPdkjAl-fX"
      },
      "source": [
        "### Visualizando gráfico de perda durante o treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T22:39:51.244869Z",
          "start_time": "2017-11-24T22:39:50.995188Z"
        },
        "id": "_fSYP_KXl-fY"
      },
      "source": [
        "plt.plot(epochs, losses)\n",
        "plt.xlabel('época')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNyGHxAtl-fc"
      },
      "source": [
        "### Visualização usual da perda, somente no final de cada minibatch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T22:39:51.498640Z",
          "start_time": "2017-11-24T22:39:51.246714Z"
        },
        "id": "e2J2aZZVl-fd"
      },
      "source": [
        "plt.plot(epochs[:5 * n_batches_train +1:n_batches_train], losses[:5 * n_batches_train+1:n_batches_train])\n",
        "plt.xlabel('época')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHLCXNLml-fg"
      },
      "source": [
        "# Atividades"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCs38MOml-fh"
      },
      "source": [
        "## Perguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffg58gb7l-fi"
      },
      "source": [
        "1. Qual é o tamanho do mini-batch?\n",
        "R. 600\n",
        "2. Em uma época, quantos mini-batches existem?\n",
        "R. 100 considerando o dataset de 60.000 dados\n",
        "3. Qual é a definição de época?\n",
        "R.Se trata da varredura completa por todos os dados de treinamento para ajuste dos pesos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DourBpfrl-fl"
      },
      "source": [
        "## Exercícios\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjh3AaNpl-fn"
      },
      "source": [
        "1. Coloque um print no final de cada minibatch, no mesmo estilo do print do final de época, no seguinte estilo:\n",
        "    - Época: 1/4, batch: 3/10\n",
        "R. print(f'Época: {i}/{n_epochs - 1}, batch: {len(loader_train)}')\n",
        "2. Altere o tamanho de minibatch (batch_size) algumas vezes, refaça o treinamento, e compare no gráfico abaixo a queda da perda para cada tamanho de minibatch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dite8MBJl-fn"
      },
      "source": [
        "## Conclusões sobre os experimentos deste notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbMsRflll-fp"
      },
      "source": [
        "1. O mini-batch contribui com a rapidez da convergência, num mesmo número de épocas\n",
        "2. Os resultado pode ser mais ruidoso\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXn9N0Cal-fq"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}